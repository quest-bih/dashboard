library(assertthat)
library(haven)
library(tidyverse)
library(vroom)
library(here)
library(easyRPubMed)
library(janitor)
library(readxl)

#----------------------------------------------------------------------------------------
# load results
#----------------------------------------------------------------------------------------

publications <- read_csv(here("main", "publication_table_old.csv")) |>
  mutate(doi = tolower(doi))
publications |>
  count(year)

dupes <- get_dupes(publications, doi)


publications_2022 <- read_xlsx(here("main", "Publikationsdatensatz CharitÃ©_2022_230922VN.xlsx")) |>
  filter(!str_detect(Duplikat, "erscheint") | is.na(Duplikat),
         is.na(Keine_Charite_Affiliation)) |>
  transmute(doi = tolower(DOI),
         pmid = `Pubmed Id`,
         title = `Article Title`,
         journal = `Source Title`,
         year = case_when(
           str_detect(`Early Access Date`, " ") ~ year(my(`Early Access Date`)),
           !str_detect(`Early Access Date`, " ") ~ as.numeric(`Early Access Date`) |>
             as.Date(origin = "1899-12-30") |>
             year(),
           str_detect(Duplikat, "halb") ~ year(`Date of Export`) - 1,
           .default = `Publication Year`
         ),
         publisher = Publisher,
         issn = ISSN,
         e_issn = eISSN,
         oa_indicator = `Open Access Designations`,
         oa_status = `OA-Status`) |>
  select(all_of(names(publications))) |>
  filter(year < 2023)

dupes <- get_dupes(publications_2022, doi)

publications_2022 |>
  count(year)

only_new_dois <- publications_2022 |>
  filter(!doi %in% publications$doi |
        str_detect(doi, "keine"))

publications <- publications |>
  rows_append(only_new_dois) |>
  # filter(str_detect(doi, "10"), doi %in% publications_2022$doi) |>
  rows_upsert(publications_2022 |>
                filter(str_detect(doi, "10")) |>
                select(doi, oa_status), by = "doi")

count(publications, year)

corresponding_authorship <- read_csv("T:/Dokumente/publications_CA.csv")

publications <- publications |>
  left_join(corresponding_authorship, by = "doi")


write_excel_csv(publications, here("main", "publication_table.csv"))


publications <- read_csv(here("main", "publication_table.csv"))

#results files
# results_files <- list.files(here("results"), full.names = TRUE)
# results_files <- paste0(results_folder, results_files)

open_data_22 <- read_csv(here("results", "Open_Data.csv")) |>
  mutate(doi = str_replace_all(article, "\\+", "\\/") |>
           str_remove(".txt") |>
           tolower()) |>
  select(doi, everything(), -article)



#manually checked Open Data results + additional cases that were not detected by algorithm
#but found in manual searches or submitted by researchers for the LOM


open_data_22_manual <- read_csv2(here("results", "OD_manual_tidy.csv")) |>
  mutate(doi = tolower(doi)) |>
  select(doi, open_data_manual_check, open_data_category_manual, data_access)

open_code_22_manual <- read_xlsx(here("results", "oddpub_code_results_manual.xlsx")) |>
  select(doi, contains("code")) |>
  mutate(is_open_code = as.logical(is_open_code),
         open_code_manual_check = as.logical(open_code_manual_check))

# dupes <- open_data_22 |>
#   filter(doi %in% manual_code_results$doi)
# publications |>
#   filter(doi %in% dupes$doi,
#          year == 2022)
# old template had some MDC papers that were later excluded

open_data_results <- read_csv2(here("results", "Open_Data_manual_check_template.csv")) |>
  mutate(is_reuse = NA,
         das = as.character(das),
         cas = as.character(cas)) |>
  rows_upsert(open_data_22, by = "doi") |>
  rows_upsert(open_code_22_manual, by = "doi") |>
  rows_upsert(open_data_22_manual, by = "doi") |>
  mutate(open_code_category_manual = case_when(
    str_detect(open_code_category_manual, "github") ~ "github",
    str_detect(open_code_category_manual, "supplement") ~ "supplement",
    open_code_manual_check == TRUE ~ "other repository/website"
  ),
  open_code_manual_check = case_when(
    str_detect(open_code_category_manual, "supplement") ~ FALSE,
    .default = open_code_manual_check),
  open_data_manual_check = case_when(
    str_detect(open_data_category_manual, "supplement") ~ FALSE,
    .default = open_data_manual_check),
  restrictions = case_when(
    str_detect(data_access, "yes") & str_detect(data_access, "restricted") ~ "partial",
    str_detect(data_access, "restricted") ~ "full",
    .default = "no restricted data"
  )
  )


# when finished update the template to contain the data from previous year
# open_data_results |>
#   write_excel_csv2(here("results", "Open_Data_manual_check_template.csv"))

# template_miss <- template |>
#   filter(!doi %in% open_data_results$doi)



old_das_cas <- vroom(here("results", "Open_Data_retroactive.csv")) |>
  select(doi, das, cas)

open_data_results <- open_data_results |>
  rows_upsert(old_das_cas, by = "doi")

open_data_results |>
  filter(is.na(is_open_data),
         open_data_category_manual == "disciplinary and general-purpose repositories") |>
  pull(doi)

open_data_results |>
  filter(open_data_manual_check) |>
  count(is_open_data, open_data_category_manual)

open_data_results |>
  count(is_open_code, open_code_manual_check)

open_data_results |>
  count(open_data_manual_check, open_data_category_manual)

#ContriBOT results
contribot_results <- read_csv(here("results", "ContriBOT.csv"))
contribot_results_retro <- read_csv(here("results", "ContriBOT_retroactive.csv"))
contribot_results <- contribot_results_retro |>
  rows_upsert(contribot_results, by = "doi")

orcid_screening_results <- read_csv(here("results", "orcids_extracted.csv"))
orcid_screening_results_retro <- read_csv(here("results", "orcids_retroactive.csv")) |>
  mutate(doi = str_extract(file, "10\\..*") |>
           str_remove(".pdf") |>
           str_replace_all("\\+", "/")) |>
  select(-file)
orcid_screening_results <- orcid_screening_results_retro |>
  rows_upsert(orcid_screening_results, by = "doi")

#Barzooka results
barzooka_results <- read_csv(here("results", "Barzooka.csv"))

barzooka_results <- barzooka_results |>
  rename(doi = paper_id) |>
  select(doi, bar, pie, bardot, box, dot, hist, violin) |>
  distinct(doi, .keep_all = TRUE)

missing_dois <- setdiff(contribot_results$doi, publications$doi)

publications |>
  filter(doi %in% missing_dois)

#----------------------------------------------------------------------------------------
# combine results
#----------------------------------------------------------------------------------------

#check if there are no duplicated dois in the results files (problem for left_join)
assert_that(length(open_data_results$doi) == length(unique(open_data_results$doi)))
assert_that(length(barzooka_results$doi) == length(unique(barzooka_results$doi)))


dashboard_metrics <- publications |>
  left_join(open_data_results, by = "doi") |>
  left_join(contribot_results, by = "doi") |>
  left_join(orcid_screening_results, by = "doi") |>
  rename(oa_color = oa_status) |>
  mutate(has_any_orcid = has_orcid | !is.na(orcids)) |>
  left_join(barzooka_results, by = "doi") |>
  mutate(pdf_downloaded = !is.na(bar) | !is.na(has_contrib))

dashboard_metrics |>
  filter(pdf_downloaded == TRUE) |>
  count(has_any_orcid, has_orcid)

orcid_screening_results |>
  count(is.na(orcids))


per_year <- dashboard_metrics |>
  count(year, open_data_category_manual) |>
  group_by(year) |>
  mutate(perc = n / sum(n) * 100)

per_year <- dashboard_metrics |>
  count(year, open_code_category_manual) |>
  group_by(year) |>
  mutate(perc = n / sum(n) * 100)


oc_dois <- open_code_22_manual |>
  filter(open_code_manual_check == TRUE,
         open_code_category_manual != "supplement") |>
  pull(doi)

dashboard_metrics |>
  filter(doi %in% oc_dois) |>
  count(year, open_code_manual_check)

# missing_dois <- setdiff(oc_dois, dashboard_metrics$doi)

no_orcid_hyperlinks <- dashboard_metrics |>
  filter(has_orcid, is.na(orcids))

# miss_year <- dashboard_metrics |>
#   filter(is.na(year))
# dashboard_metrics |>
#   count(OA_color)

#----------------------------------------------------------------------------------------
# further preprocessing & validation steps
#----------------------------------------------------------------------------------------

#data plausibility/quality check for the updated PDF dataset -> are there any new
#cases that we missed? For old PDF dataset, there were 0 cases, now again 0 cases
check_tbl <- dashboard_metrics |>
  filter(
    (is_open_data & is.na(open_data_manual_check)) |
           (is_open_code & is.na(open_code_manual_check))) |>
  select(doi, year, is_open_data, open_data_category, is_open_code,
         open_data_statements, open_code_statements,
         open_data_manual_check, open_data_category_manual,
         open_code_manual_check, open_code_category_manual)

# check_tbl |>
#   filter(doi %in% c("10.1126/scitranslmed.abq3010",
#                     "10.18332/ejm/150582"))

# debug ODDPUb until these false_positives are is_open_data == FALSE!!!
# for now set to false manually
dashboard_metrics <- dashboard_metrics |>
  mutate(is_open_data = case_when(
    doi %in% check_tbl$doi ~ FALSE,
    .default = is_open_data
  ))



assert_that(dim(check_tbl)[1] == 0)
 # write_csv(check_tbl, "./results/pdf_update_cases.csv")


#----------------------------------------------------------------------------------------
# save resulting table with relevant columns only
#----------------------------------------------------------------------------------------

#only select columns relevant for shiny table
shiny_table <- dashboard_metrics |>
  select(doi, pmid, title, journal, year,
         publisher, issn, e_issn,
         corresponding_author_charite,
         pdf_downloaded, oa_color,
         is_open_data, open_data_manual_check, open_data_category_manual,
         is_open_code, open_code_manual_check, open_code_category_manual,
         open_data_statements, open_code_statements,
         das, cas,
         restrictions,
         has_contrib, contrib_statement,
         has_orcid, orcid_statement,
         orcids, has_any_orcid,
         bar, pie, bardot, box, dot, hist, violin)

write_csv(shiny_table, here("shiny_app", "data", "dashboard_metrics.csv"))

shiny_table |>
  count(year, restrictions)

# dashboard_metrics |>
#   count(year, is.na(is_open_data))
#
# dashboard_metrics |>
#   count(year, is.na(has_contrib))
#
# year_21 <- wrong_years |>
#   filter(year == 2021)
# year_22 <- dashboard_metrics |>
#   filter(year == 2022)
#
# year_22 |>
#   count(is.na(has_contrib))
#
# nas_22 <- year_22 |>
#   filter(is.na(has_contrib))
#
# wrong_years <- dashboard_metrics |>
#   filter(year != 2022,
#          !is.na(has_contrib))

# dashboard_metrics <- read_csv(here("shiny_app", "data", "dashboard_metrics.csv"))
#----------------------------------------------------------------------------------------
# now for the metrics that are already aggregated by year
#----------------------------------------------------------------------------------------

preprints_dataset_shiny <- vroom(here("results", "preprints_oa.csv")) |>
  select(doi, title, journal_title, year)
write_csv(preprints_dataset_shiny, here("shiny_app", "data", "preprints_dataset_shiny.csv"))

prosp_reg_dataset_shiny <- vroom(here("results", "prosp_reg_dataset_shiny.csv")) |>
  rename(registration_date = study_first_submitted_date) |>
  select(nct_id, start_date, registration_date, has_prospective_registration)
write_csv(prosp_reg_dataset_shiny, here("shiny_app", "data", "prosp_reg_dataset_shiny.csv"))

# orcid_dataset_shiny <- vroom(here("results", "orcid.csv")) |>
#   distinct(date, .keep_all = TRUE) |> # next part only if false parsing in file with spliced rows
#   mutate(orcid_count = if_else(str_length(orcid_count) > 5, str_sub(orcid_count, 1, 4),
#                                as.character(orcid_count)),
#          orcid_count = as.numeric(orcid_count))
# write_csv(orcid_dataset_shiny, here("shiny_app", "data", "orcid_results.csv"))

EU_trialstracker_dataset_shiny <- vroom(here("results", "EU_trialstracker.csv")) |>
  group_by(retrieval_date) |>
  slice(1) |>
  mutate(perc_reported = round(total_reported/total_due, 3)) |>
  arrange(desc(retrieval_date))
write_csv(EU_trialstracker_dataset_shiny, here("shiny_app", "data", "EU_trialstracker_past_data.csv"))

preprints <- vroom(here("results", "preprints_oa.csv")) |>
  group_by(year) |>
  summarize(n_preprints = n())

prospective_registration <- vroom(here("results", "prospective_registration.csv"))

max_year <- max(prospective_registration$year)

shiny_table_aggregate_metrics <- tibble(year = 2006:max_year) |>
  left_join(prospective_registration) |>
  left_join(preprints)

write_csv(shiny_table_aggregate_metrics, here("shiny_app", "data", "dashboard_metrics_aggregate.csv"))

#----------------------------------------------------------------------------------------
# fair assessment
#----------------------------------------------------------------------------------------

fair_table <- vroom(here("results", "fair_assessment_2021.csv"), show_col_types = FALSE)
write_csv(fair_table, here("shiny_app", "data", "fair_assessment_2021.csv"))

#----------------------------------------------------------------------------------------
# berlin science survey
#----------------------------------------------------------------------------------------

bss_stata <- read_dta(here("results", "bss-pilot22-char.dta"))
write_dta(bss_stata, here("shiny_app", "data", "bss-pilot22-char.dta"))

